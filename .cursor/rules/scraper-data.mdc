---
description: Web Scraper Guidelines
globs: src/scraper/*.py, src/routers/scraper.py, src/schemas/financial_data.py
alwaysApply: false
---
---
description: Web Scraper Guidelines
globs: src/scraper/*.py, src/routers/scraper.py, src/schemas/financial_data.py
alwaysApply: false
---

# Web Scraper Guidelines

## Purpose
This rule provides guidance on implementing and managing web scrapers for financial data collection in the StockAnalysis application.

## Key Principles
- ✓ Define appropriate data models for scrape requests and results
- ✓ Implement scraper services with proper error handling
- ✓ Follow ethical scraping practices (respect robots.txt, rate limiting)
- ✓ Process and validate scraped data before storage
- ✓ Organize scraper code in the appropriate directory structure
- ✓ Implement multiple fallback strategies for data extraction
- ✓ Properly manage resources (WebDriver, connections)

## Data Structures
```python
class ScrapeRequest(BaseModel):
    result_type: str  # "LR", "BP", "WP", "PT", "NT"
    url: Optional[str] = None

class ScrapedFinancialData(BaseModel):
    company_name: str
    symbol: Optional[str] = None
    result_date: str
    financial_metrics: Dict[str, Any]
    quarter: str
    source: str
```

## Robust Data Extraction
```python
def extract_stock_symbol(driver, url, company_name):
    """Extract stock symbol with multiple fallback strategies."""
    try:
        # Strategy 1: Try main selector
        symbol_element = driver.find_element(By.CSS_SELECTOR, "main-selector")
        if symbol_element:
            return symbol_element.text.strip()
            
        # Strategy 2: Try alternative selector
        symbol_element = driver.find_element(By.CSS_SELECTOR, "alt-selector")
        if symbol_element:
            return symbol_element.text.strip()
            
        # Strategy 3: Extract from breadcrumbs
        breadcrumbs = driver.find_elements(By.CSS_SELECTOR, "breadcrumb-selector")
        if breadcrumbs:
            # Process breadcrumbs
            pass
            
        # Strategy 4: Parse from page title
        title = driver.title
        if "(" in title and ")" in title:
            return title.split("(")[1].split(")")[0].strip()
            
        # Strategy 5: Format company name as symbol
        return format_as_symbol(company_name)
            
    except Exception as e:
        logger.warning(f"Error extracting symbol: {e}")
        # Return a reasonable fallback
        return format_as_symbol(company_name)
```

## Resource Management
```python
driver = None
try:
    # Set up the driver
    driver = setup_webdriver()
    
    # Use the driver
    driver.get(url)
    wait = WebDriverWait(driver, 30)
    element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "selector")))
    
    # Extract and process data
    data = extract_data(driver)
    
except Exception as e:
    logger.error(f"Scraping error: {e}")
    return None
finally:
    # Clean up resources
    if driver:
        try:
            driver.quit()
        except Exception as e:
            logger.warning(f"Error closing WebDriver: {e}")
```

## API Endpoints
- `POST /api/v1/scraper/moneycontrol` - Scrape data from MoneyControl
- `GET /api/v1/market/quarters` - Get available quarters

## File Organization
- ✓ Modules: `src/scraper/`
- ✓ Utilities: `src/scraper/utils.py`
- ✓ API endpoints: `src/routers/scraper.py`
- ✓ Data schemas: `src/schemas/financial_data.py`
- ✓ Tests: `tests/test_scraper.py`

## Implementation Guidelines
- ✓ Use BeautifulSoup for HTML parsing
- ✓ Use Selenium for JavaScript-heavy pages
- ✓ Implement retry logic for transient failures
- ✓ Handle rate limiting and blocking
- ✓ Log all scraping operations for traceability
- ✓ Take screenshots for debugging visual elements
- ✓ Implement explicit waits for dynamic content

## Data Processing
- ✓ Clean and normalize scraped data
- ✓ Convert string values to appropriate types
- ✓ Validate data structure before storage
- ✓ Store raw data for debugging purposes
- ✓ Handle missing or inconsistent data gracefully
- ✓ Implement multiple data extraction strategies
- ✓ Log data extraction steps for debugging

## Debugging Techniques
- ✓ Save screenshots at critical points
- ✓ Log the DOM state when extraction fails
- ✓ Implement debug mode with verbose logging
- ✓ Test with headless and headed browser modes
- ✓ Log extracted values before processing

These guidelines ensure ethical, effective, and robust web scraping for financial data collection.